{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sen3r-NcExplorer: from mpl_toolkits.basemap import Basemap FAILED!\n",
      "You can still proceed without plotting any maps.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from tsgen import TsGenerator\n",
    "from nc_explorer import NcExplorer\n",
    "\n",
    "tsgen = TsGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting RAW .SEN3 NetCDF4 without SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = {\n",
    "#     '/d_drive_data/S3/stations/12350000_fonte_boa/': ['/d_drive_data/S3/stations/12350000_fonte_boa_img/','/d_drive_data/A1_JM/areas/new_areas/12350000_fonte_boa.geojson'],\n",
    "#     '/d_drive_data/S3/stations/12900001_tefe/': ['/d_drive_data/S3/stations/12900001_tefe_img/','/d_drive_data/A1_JM/areas/new_areas/12900001_tefe.geojson'],\n",
    "#     '/d_drive_data/S3/stations/13100090_coari/': ['/d_drive_data/S3/stations/13100090_coari_img/','/d_drive_data/A1_JM/areas/new_areas/13100090_coari.geojson'],\n",
    "#     '/d_drive_data/S3/stations/15860000_borba_madeira/': ['/d_drive_data/S3/stations/15860000_borba_madeira_img/','/d_drive_data/A1_JM/areas/new_areas/15860000_borba_madeira.geojson'],\n",
    "#     '/d_drive_data/S3/stations/17050001_obidos/': ['/d_drive_data/S3/stations/17050001_obidos_img/','/d_drive_data/A1_JM/areas/new_areas/17050001_obidos.geojson']\n",
    "#     '/d_drive_data/S3/stations/14910000_negro/': ['/d_drive_data/S3/stations/','/d_drive_data/A1_JM/areas/new_areas/14910000_negro.geojson'],\n",
    "#     '/d_drive_data/S3/stations/14900050_negro_2/': ['/d_drive_data/S3/stations/','/d_drive_data/A1_JM/areas/new_areas/14900050_negro_2.geojson']\n",
    "# }\n",
    "stations = {\n",
    "    '/d_drive_data/S3/stations/AN1/': ['/d_drive_data/S3/stations/','/d_drive_data/A1_JM/artigo_SEN3R/ROIs/AN1.geojson'],\n",
    "    '/d_drive_data/S3/stations/BCO/': ['/d_drive_data/S3/stations/','/d_drive_data/A1_JM/artigo_SEN3R/ROIs/BCO.geojson']\n",
    "#     '/d_drive_data/S3/stations/14100000_manacapuru/': ['/d_drive_data/S3/stations/14100000_manacapuru/','/d_drive_data/A1_JM/areas/paper_areas/14100000_manacapuru.geojson']\n",
    "#     '/d_drive_data/S3/stations/14900050_negro_19/': ['/d_drive_data/S3/stations/','/d_drive_data/A1_JM/areas/paper_areas/14900050_negro_19.geojson'],\n",
    "#     '/d_drive_data/S3/stations/14900050_negro_29/': ['/d_drive_data/S3/stations/','/d_drive_data/A1_JM/areas/paper_areas/14900050_negro_29.geojson'],\n",
    "#     '/d_drive_data/S3/stations/14900050_negro_37/': ['/d_drive_data/S3/stations/','/d_drive_data/A1_JM/areas/paper_areas/14900050_negro_37.geojson']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stations:\n",
    "    \n",
    "    # LINUX\n",
    "    in_dir = '/d_drive_data/S3/L2_WFR'\n",
    "    out_dir = s\n",
    "    img_dir = stations[s][0]\n",
    "    poly = stations[s][1]\n",
    "    \n",
    "    todo = tsgen.build_list_from_subset(in_dir)\n",
    "    todo_fullpath = [os.path.join(in_dir,csv) for csv in todo]\n",
    "    total = len(todo_fullpath)\n",
    "    \n",
    "    info_string = f'Processing {total} files for station: {s} using polygon: {stations[s][1]}'\n",
    "    print(info_string)\n",
    "    os.system(f'telegram-send \\\"{info_string}\\\"')\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    for n, img in enumerate(todo_fullpath):\n",
    "\n",
    "        f_b_name = os.path.basename(img).split('.')[0]\n",
    "        print(f'>>> Processing: {n+1} of {total} ... {f_b_name}')\n",
    "        \n",
    "        if os.path.isfile(os.path.join(out_dir, f_b_name+'.csv')):\n",
    "            print('Skipped.')\n",
    "            continue\n",
    "        \n",
    "        ncxp = NcExplorer(input_nc_folder=img, product='wfr')\n",
    "\n",
    "        df = ncxp.get_data_in_poly(poly_path=poly, go_parallel=False)\n",
    "\n",
    "        # if df is not None:\n",
    "        print(f'Saving DF: {f_b_name}')\n",
    "        df.to_csv(os.path.join(out_dir, f_b_name+'.csv'), index=False)\n",
    "\n",
    "    t2 = time.perf_counter()\n",
    "    outputstr = f'>>> Finished in {round(t2 - t1, 2)} second(s). <<<'\n",
    "    print(outputstr)\n",
    "    os.system(f'telegram-send \\\"{outputstr}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating post-processed CSVs from the raw extracted CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Folder of files containing raw pixels extracted with GPT/SEN3R\n",
    "* Folder to save the processed files\n",
    "* Folder in which the plots should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/d_drive_data/A1_JM/artigo_SEN3R/ROIs/SHP/AN1.shp',\n",
       " '/d_drive_data/A1_JM/artigo_SEN3R/ROIs/SHP/BCO.shp']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # retrieve all files in folder\n",
    "# shps_path = '/d_drive_data/A1_JM/areas/paper_areas/shp'\n",
    "# files = os.listdir(shps_path)\n",
    "# # extract only the Shapefiles from the list\n",
    "# shps = [os.path.join(shps_path,f) for f in files if f.endswith('.shp')]\n",
    "# shps\n",
    "\n",
    "# shps = ['/d_drive_data/A1_JM/areas/paper_areas/shp/14910000_negro.shp']\n",
    "# shps = ['/d_drive_data/A1_JM/areas/paper_areas/shp/14100000_manacapuru.shp']\n",
    "# shps = ['/d_drive_data/A1_JM/areas/paper_areas/shp/14900050_negro_19.shp',\n",
    "#         '/d_drive_data/A1_JM/areas/paper_areas/shp/14900050_negro_29.shp',\n",
    "#         '/d_drive_data/A1_JM/areas/paper_areas/shp/14900050_negro_37.shp']\n",
    "shps = ['/d_drive_data/A1_JM/artigo_SEN3R/ROIs/SHP/AN1.shp',\n",
    "        '/d_drive_data/A1_JM/artigo_SEN3R/ROIs/SHP/BCO.shp']\n",
    "shps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = '/d_drive_data/S3/stations/'\n",
    "dest = '/d_drive_data/processing/linux/'\n",
    "\n",
    "for station in shps:\n",
    "    \n",
    "    procstring = f'processing: {station}'\n",
    "    print(procstring)\n",
    "    os.system(f'telegram-send \\\"{procstring}\\\"')\n",
    "    \n",
    "#     # GET SHP CENTROIDS\n",
    "#     polygons = gpd.GeoDataFrame.from_file(station)\n",
    "#     polygons.geometry = polygons.representative_point()\n",
    "#     centroids = []\n",
    "#     for g in polygons['geometry']:\n",
    "#         q_lon, q_lat = g.coords[:][0]\n",
    "#         centroids.append((q_lon, q_lat))\n",
    "    \n",
    " \n",
    "    # GET SERIES SAVE PATH\n",
    "    station_name =    os.path.basename(station).split('.')[0]\n",
    "    in_dir =          os.path.join(raw_dir,station_name)\n",
    "    out_dir =         os.path.join(dest,station_name+f'_v{version}')\n",
    "    img_dir =         os.path.join(dest,station_name+f'_v{version}_img')\n",
    "    img_save_pth =    os.path.join(dest,station_name+f'_v{version}_img_dbscan')\n",
    "    series_save_pth = os.path.join(dest,station_name+f'_v{version}_img_dbscan_series')\n",
    "\n",
    "\n",
    "    # CREATE THE DIRECTORIES IF THEY DOESN'T EXIST YET\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(img_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(img_save_pth).mkdir(parents=True, exist_ok=True)\n",
    "    Path(series_save_pth).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # GET LIST OF CSV FILES\n",
    "    todo = tsgen.build_list_from_subset(in_dir)\n",
    "    todo_fullpath = [os.path.join(in_dir,csv) for csv in todo]\n",
    "    \n",
    "    # Start timer\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    # Desclaring lists for further validation\n",
    "    checklist = {}\n",
    "    skiplst = []\n",
    "    donelst = []\n",
    "    \n",
    "    # irmax = 0.001 # Negro\n",
    "    irmax = 0.2 # Generic\n",
    "    # irmax = 0.08 # Fonte Boa\n",
    "    # irmin = 0.001 # Manacapuru\n",
    "\n",
    "\n",
    "    total = len(todo_fullpath)\n",
    "    for n,img in enumerate(todo_fullpath):\n",
    "        print(f'>>> Processing: {n+1} of {total} ... {img}')\n",
    "\n",
    "        # reprocessing the raw CSVs and removing \n",
    "        # reflectances above the threshold in IR.\n",
    "\n",
    "        try:\n",
    "            dfpth, df = tsgen.update_csvs(csv_path=img,\n",
    "                                          glint=20.0,\n",
    "                                          #ir_min_threshold=irmin,\n",
    "                                          ir_max_threshold=irmax,\n",
    "                                          savepath=out_dir)\n",
    "        except Exception as e:\n",
    "            print(\"type error: \" + str(e))\n",
    "            skiplst.append(img)\n",
    "            continue\n",
    "\n",
    "        # The KDE needs at least two different reflectance values to work.\n",
    "        if dfpth == 'KDE_fail':\n",
    "            skiplst.append(img)\n",
    "            continue\n",
    "\n",
    "    #     try:\n",
    "    #         dfpth, df = tsgen.update_csvs(csv_path=img,savepath=out_dir,threshold=0.2)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"type error: \" + str(e))\n",
    "    #         print(f'Skipping CSV for lack of data: {figdate}')\n",
    "    #         skiplst.append(img)\n",
    "    #         continue\n",
    "\n",
    "    #     dfpth, df = tsgen.update_csvs(csv_path=img,\n",
    "    #                                   savepath=out_dir,\n",
    "    #                                   threshold=0.2)\n",
    "\n",
    "        if len(df) < 1:\n",
    "            print(f'Skipping empty CSV: {dfpth}')\n",
    "            skiplst.append(img)\n",
    "            continue\n",
    "        else:\n",
    "            donelst.append(img)\n",
    "\n",
    "\n",
    "        # fig params -------------------------------------------------\n",
    "        figdate = os.path.basename(img).split('____')[1].split('_')[0]\n",
    "        figtitl = os.path.basename(out_dir)+'_'+figdate\n",
    "        savpt_sctr = os.path.join(img_dir,figdate+'_0.png')\n",
    "        savpt_rrs = os.path.join(img_dir,figdate+'_1.png')\n",
    "\n",
    "        # generate plot v1 --------------------------\n",
    "    #     tsgen.plot_single_sktr(\n",
    "    #         xdata=df['Oa08_reflectance:float'],\n",
    "    #         ydata=df['Oa21_reflectance:float'],\n",
    "    #         xlabel='Oa08: 665 nm',\n",
    "    #         ylabel='Oa17: 865 nm',\n",
    "    #         color=df['T865:float'],\n",
    "    #         clabel='T865',\n",
    "    #         title=figtitl,\n",
    "    #         savepathname=savpt)\n",
    "\n",
    "        # generate plot v3 --------------------------\n",
    "        tsgen.plot_sidebyside_sktr(x1_data=df['Oa08_reflectance:float'],\n",
    "                                   y1_data=df['Oa17_reflectance:float'],\n",
    "                                   x2_data=df['Oa08_reflectance:float'],\n",
    "                                   y2_data=df['Oa17_reflectance:float'],\n",
    "                                   x_lbl='RED: Oa08 (665nm)',\n",
    "                                   y_lbl='NIR: Oa17 (865nm)',\n",
    "                                   c1_data=df['A865:float'],\n",
    "                                   c1_lbl='Aer. Angstrom Expoent (A865)',\n",
    "                                   c2_data=df['T865:float'],\n",
    "                                   c2_lbl='Aer. Optical Thickness (T865)',\n",
    "                                   title=f'{os.path.basename(out_dir)} WFR {figdate} RED:Oa08(665nm) x NIR:Oa17(865nm)',\n",
    "                                   savepathname=savpt_sctr)\n",
    "\n",
    "        tsgen.s3l2_custom_reflectance_plot(df=df,\n",
    "                                           figure_title=figdate,\n",
    "                                           c_lbl='Aer. Optical Thickness (T865)',\n",
    "                                           save_title=savpt_rrs)\n",
    "\n",
    "    t2 = time.perf_counter()\n",
    "    outputstr = f'>>> Finished in {round(t2 - t1, 2)} second(s). <<<'\n",
    "    print(outputstr)\n",
    "    os.system(f'telegram-send \\\"{outputstr}\\\"')\n",
    "    \n",
    "    # Generating excel file from the post-processed data\n",
    "    wdir = out_dir\n",
    "    todo = tsgen.build_list_from_subset(wdir)\n",
    "    todo_fullpath = [os.path.join(wdir,txt) for txt in todo]\n",
    "    \n",
    "    # Converting and saving the list of mean values into a XLS excel file.\n",
    "    data = tsgen.generate_time_series_data(wdir,todo)\n",
    "\n",
    "    series_df = pd.DataFrame(data=data)\n",
    "    # Delete these row indexes from dataFrame\n",
    "    # indexNames = series_df[series_df['B17-865'] > irmax].index\n",
    "    # indexNames = series_df[series_df['B17-865'] < irmin].index\n",
    "    # series_df.drop(indexNames, inplace=True)\n",
    "    \n",
    "    # create empty excel\n",
    "    path = os.path.join(dest,station_name+f'_v{version}.xlsx')\n",
    "    wb = openpyxl.Workbook()\n",
    "    wb.save(path)\n",
    "\n",
    "    # open the empty file and fill it up\n",
    "    sheet = f'v{version}'\n",
    "    book = openpyxl.load_workbook(path)\n",
    "    writer = pd.ExcelWriter(path, engine = 'openpyxl')\n",
    "    writer.book = book\n",
    "\n",
    "    # LINUX\n",
    "    series_df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "    writer.save()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating excel file from the post-processed data\n",
    "wdir = out_dir\n",
    "todo = tsgen.build_list_from_subset(wdir)\n",
    "todo_fullpath = [os.path.join(wdir,txt) for txt in todo]\n",
    "\n",
    "# Converting and saving the list of mean values into a XLS excel file.\n",
    "data = tsgen.generate_time_series_data(wdir,todo)\n",
    "\n",
    "series_df = pd.DataFrame(data=data)\n",
    "# Delete these row indexes from dataFrame\n",
    "# indexNames = series_df[series_df['B17-865'] > irmax].index\n",
    "# indexNames = series_df[series_df['B17-865'] < irmin].index\n",
    "# series_df.drop(indexNames, inplace=True)\n",
    "\n",
    "# create empty excel\n",
    "path = os.path.join(dest,station_name+f'_v{version}.xlsx')\n",
    "wb = openpyxl.Workbook()\n",
    "wb.save(path)\n",
    "\n",
    "# open the empty file and fill it up\n",
    "sheet = f'v{version}'\n",
    "book = openpyxl.load_workbook(path)\n",
    "writer = pd.ExcelWriter(path, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "# LINUX\n",
    "series_df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the empty file and fill it up\n",
    "sheet = f'v{version}'\n",
    "book = openpyxl.load_workbook(path)\n",
    "writer = pd.ExcelWriter(path, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "# LINUX\n",
    "series_df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new CSVs and plots based in the RAW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfpth, df = tsgen.update_csvs(csv_path=todo_fullpath[0],savepath=out_dir,threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "skiplst = []\n",
    "donelst = []\n",
    "\n",
    "total = len(todo_fullpath)\n",
    "for n,img in enumerate(todo_fullpath):\n",
    "    print(f'>>> Processing: {n+1} of {total} ... {img}')\n",
    "    \n",
    "    # reprocessing the raw CSVs and removing \n",
    "    # reflectances above the threshold in IR.\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        dfpth, df = tsgen.update_csvs(csv_path=img,savepath=out_dir,threshold=0.2,glint=20.0)\n",
    "    except Exception as e:\n",
    "        print(\"type error: \" + str(e))\n",
    "        skiplst.append(img)\n",
    "        continue\n",
    "\n",
    "    # The KDE needs at least two different reflectance values to work.\n",
    "    if dfpth == 'KDE_fail':\n",
    "        skiplst.append(img)\n",
    "        continue\n",
    "    \n",
    "#     try:\n",
    "#         dfpth, df = tsgen.update_csvs(csv_path=img,savepath=out_dir,threshold=0.2)\n",
    "#     except Exception as e:\n",
    "#         print(\"type error: \" + str(e))\n",
    "#         print(f'Skipping CSV for lack of data: {figdate}')\n",
    "#         skiplst.append(img)\n",
    "#         continue\n",
    "\n",
    "#     dfpth, df = tsgen.update_csvs(csv_path=img,\n",
    "#                                   savepath=out_dir,\n",
    "#                                   threshold=0.2)\n",
    "    \n",
    "    if len(df) < 1:\n",
    "        print(f'Skipping empty CSV: {dfpth}')\n",
    "        skiplst.append(img)\n",
    "        continue\n",
    "    else:\n",
    "        donelst.append(img)\n",
    "    \n",
    "    \n",
    "    # fig params -------------------------------------------------\n",
    "    figdate = os.path.basename(img).split('____')[1].split('_')[0]\n",
    "    figtitl = os.path.basename(out_dir)+'_'+figdate\n",
    "    savpt_sctr = os.path.join(img_dir,figdate+'_0.png')\n",
    "    savpt_rrs = os.path.join(img_dir,figdate+'_1.png')\n",
    "    \n",
    "    print(f'Generating image: {savpt_sctr}')\n",
    "    \n",
    "    # generate plot v1 --------------------------\n",
    "#     tsgen.plot_single_sktr(\n",
    "#         xdata=df['Oa08_reflectance:float'],\n",
    "#         ydata=df['Oa21_reflectance:float'],\n",
    "#         xlabel='Oa08: 665 nm',\n",
    "#         ylabel='Oa17: 865 nm',\n",
    "#         color=df['T865:float'],\n",
    "#         clabel='T865',\n",
    "#         title=figtitl,\n",
    "#         savepathname=savpt)\n",
    "    \n",
    "    # generate plot v3 --------------------------\n",
    "    tsgen.plot_sidebyside_sktr(x1_data=df['Oa08_reflectance:float'],\n",
    "                               y1_data=df['Oa17_reflectance:float'],\n",
    "                               x2_data=df['Oa08_reflectance:float'],\n",
    "                               y2_data=df['Oa17_reflectance:float'],\n",
    "                               x_lbl='RED: Oa08 (665nm)',\n",
    "                               y_lbl='NIR: Oa17 (865nm)',\n",
    "                               c1_data=df['A865:float'],\n",
    "                               c1_lbl='Aer. Angstrom Expoent (A865)',\n",
    "                               c2_data=df['T865:float'],\n",
    "                               c2_lbl='Aer. Optical Thickness (T865)',\n",
    "                               title=f'{os.path.basename(out_dir)} WFR {figdate} RED:Oa08(665nm) x NIR:Oa17(865nm)',\n",
    "                               savepathname=savpt_sctr)\n",
    "    \n",
    "    print(f'Generating image: {savpt_rrs}')\n",
    "    \n",
    "    tsgen.s3l2_custom_reflectance_plot(df=df,\n",
    "                                       figure_title=figdate,\n",
    "                                       c_lbl='Aer. Optical Thickness (T865)',\n",
    "                                       save_title=savpt_rrs)\n",
    "    \n",
    "t2 = time.perf_counter()\n",
    "outputstr = f'>>> Finished in {round(t2 - t1, 2)} second(s). <<<'\n",
    "print(outputstr)\n",
    "os.system(f'telegram-send \\\"{outputstr}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating mean values dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = out_dir\n",
    "\n",
    "todo = tsgen.build_list_from_subset(wdir)\n",
    "\n",
    "todo_fullpath = [os.path.join(wdir,txt) for txt in todo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting and saving the list of mean values into a XLS excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tsgen.generate_time_series_datav2(wdir,todo)\n",
    "\n",
    "series_df = pd.DataFrame(data=data)\n",
    "\n",
    "path = '/d_drive_data/processing/linux/manacapuru_v10.xlsx'\n",
    "# path = f'/d_drive_data/processing/linux/{os.path.basename(out_dir)}.xlsx'\n",
    "# sheet = f'{os.path.basename(out_dir)}higlint'\n",
    "sheet = f'v10g20'\n",
    "\n",
    "book = load_workbook(path)\n",
    "writer = pd.ExcelWriter(path, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "# LINUX\n",
    "series_df.to_excel(writer, sheet_name=sheet)\n",
    "\n",
    "# WIN\n",
    "# series_df.to_excel(f'D:\\\\processing\\\\win\\\\{os.path.basename(out_dir)}.xlsx', sheet_name=f'{os.path.basename(out_dir)}')\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAMS+DBSCAN processing for a list of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "\n",
    "ncxp = NcExplorer(initialize=False, external_use=True)\n",
    "\n",
    "def nc_dt2num(date):\n",
    "    # https://stackoverflow.com/questions/39997314/write-times-in-netcdf-file\n",
    "    units = \"hours since 1900-01-01 00:00:00.0\"\n",
    "    calendar = \"gregorian\"\n",
    "    return netCDF4.date2num(date, units=units, calendar=calendar)\n",
    "\n",
    "def get_cams_band_by_time(numeric_date, cams_time_array):\n",
    "    pos = np.where(cams_time_array == numeric_date)[0]\n",
    "    return pos[0]\n",
    "\n",
    "def find_cams_val(number_date, cams_time_array, query_lon=-60.8911,query_lat=-3.5726):\n",
    "    # Manacapuru Centroid\n",
    "    # query_lon, query_lat = -60.8911, -3.5726\n",
    "    # query for the corret day position inside the CAMS time array\n",
    "    day_position = get_cams_band_by_time(number_date, cams_time_array)\n",
    "    # extract CAMS AOD data at this day\n",
    "    aod_band = cams_nc.variables['aod865'][day_position][:]\n",
    "    # query the AOD pixel value over the target coordinates\n",
    "    cams_val = ncxp.get_point_data_in_single_band(aod_band, \n",
    "                                                 lon=lon_grid, \n",
    "                                                 lat=lat_grid,\n",
    "                                                 target_lon=query_lon,\n",
    "                                                 target_lat=query_lat)[1]\n",
    "    \n",
    "    return cams_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating iterable list of NC paths\n",
    "\n",
    "ncp = '/d_drive_data/CAMS/'\n",
    "ncps = os.listdir(ncp)\n",
    "ncps = [os.path.join(ncp,nc) for nc in ncps]\n",
    "\n",
    "# Storing NC data inside dicts accessible by YEAR\n",
    "\n",
    "lons = {}\n",
    "lats = {}\n",
    "cm_time = {}\n",
    "cams_nc = {}\n",
    "lon_grid = {}\n",
    "lat_grid = {}\n",
    "\n",
    "for ncp in ncps:\n",
    "    \n",
    "    print(f'extracting data for file: {ncp}')\n",
    "    year = int(os.path.basename(ncp).split('.')[0].split('_')[1])\n",
    "    cams_nc[year] = netCDF4.Dataset(ncp,'r')\n",
    "\n",
    "    # get the longitude information\n",
    "    lons[year] = cams_nc[year].variables['longitude'][:]\n",
    "    # get the latitude information\n",
    "    lats[year] = cams_nc[year].variables['latitude'][:]\n",
    "    # extract the time dimension\n",
    "    cm_time[year] = cams_nc[year].variables['time']\n",
    "\n",
    "    # creating 2D grids for further plotting\n",
    "    lon_grid[year], lat_grid[year] = np.meshgrid(lons[year], lats[year])\n",
    "\n",
    "cm_time.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
